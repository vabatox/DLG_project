{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/weather.20160201.csv', '/home/jovyan/weather.20160301.csv']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all weather csv files\n",
    "weather_files = sorted(glob(os.getcwd()+'/weather*.csv'))\n",
    "weather_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the weather files\n",
    "weather_concat = pd.concat((pd.read_csv(file).assign(filename=file) for file in weather_files),ignore_index=\n",
    "          True)\n",
    "#check weather_concat\n",
    "#weather_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to parquet\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "table = pa.Table.from_pandas(weather_concat)\n",
    "\n",
    "pq.write_table(table, 'weather.parquet')\n",
    "#print(pq.read_table('weather.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        Hottest_Day|\n",
      "+-------------------+\n",
      "|2016-02-01T00:00:00|\n",
      "+-------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|avg_temp_across_regions|\n",
      "+-----------------------+\n",
      "|      9.438027233477255|\n",
      "+-----------------------+\n",
      "\n",
      "+---------------+-------------------+------------------+\n",
      "|         Region|    ObservationDate|          avg_temp|\n",
      "+---------------+-------------------+------------------+\n",
      "|East of England|2016-02-21T00:00:00|12.000595238095238|\n",
      "+---------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the parquet file\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"weather_data\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "# to read parquet file\n",
    "df = sqlContext.read.parquet('weather.parquet')\n",
    "\n",
    "#check columns & data types\n",
    "#df.columns\n",
    "#df.dtypes\n",
    "\n",
    "#check count\n",
    "df.createOrReplaceTempView(\"weather\")\n",
    "#sql_results = spark.sql(\"SELECT count(1) FROM weather limit 10\")\n",
    "#sql_results.show()\n",
    "##194697\n",
    "\n",
    "#Which date was the hottest day? \n",
    "##not counting null pressure as ScreenTemperature = -99\n",
    "q1 = \"\"\"\n",
    "select\n",
    "ObservationDate AS Hottest_Day\n",
    "from\n",
    "(\n",
    "SELECT \n",
    "ObservationDate,\n",
    "--sum(cast(ScreenTemperature as float))\n",
    "avg(ScreenTemperature)\n",
    "FROM weather\n",
    "where \n",
    "Pressure is not null\n",
    "group by \n",
    "ObservationDate\n",
    "order by 2 desc\n",
    "limit 1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql_results = spark.sql(q1)\n",
    "#output to /tmp/hottest_day/\n",
    "sql_results.coalesce(1).write.format('csv').mode(\"overwrite\").save(\"/tmp/hottest_day\", header='true')\n",
    "\n",
    "sql_results.show()\n",
    "\n",
    "#+-------------------+\n",
    "#|        Hottest_Day|\n",
    "#+-------------------+\n",
    "#|2016-02-01T00:00:00|\n",
    "#+-------------------+\n",
    "\n",
    "#What was the temperature on that day? \n",
    "\n",
    "#Hottest_Day1 = sql_results.collect()\n",
    "#print(Hottest_Day1)\n",
    "\n",
    "q2 = \"\"\"\n",
    "SELECT \n",
    "avg(ScreenTemperature) as avg_temp_across_regions\n",
    "FROM weather\n",
    "where \n",
    "Pressure is not null and\n",
    "ObservationDate = '2016-02-01T00:00:00'\n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "sql_results2 = spark.sql(q2)\n",
    "#output to /tmp/hottest_day_temp/\n",
    "sql_results2.coalesce(1).write.format('csv').mode(\"overwrite\").save(\"/tmp/hottest_day_temp\", header='true')\n",
    "sql_results2.show()\n",
    "\n",
    "# In which region was the hottest day? \n",
    "q3 = \"\"\"\n",
    "SELECT \n",
    "Region,\n",
    "ObservationDate,\n",
    "avg(ScreenTemperature) as avg_temp\n",
    "FROM weather\n",
    "where \n",
    "Pressure is not null\n",
    "group by\n",
    "Region,\n",
    "ObservationDate\n",
    "order by 3 desc\n",
    "\n",
    "limit 1\n",
    "\"\"\"\n",
    "\n",
    "sql_results3 = spark.sql(q3)\n",
    "#output to /tmp/hottest_day_region/\n",
    "sql_results3.coalesce(1).write.format('csv').mode(\"overwrite\").save(\"/tmp/hottest_day_region\", header='true')\n",
    "sql_results3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
